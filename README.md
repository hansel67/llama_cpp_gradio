# llama_cpp_gradio

**Description:**  
This is a simple implementation of Llama.cpp using the gradio WebUI Python library, which allows you to run local quantized LLMs which are in the form of GGUF files. Simply install the Python packages, download the desired GGUF files from llama.meta.com, and run the app using:

```
python app.py
```
